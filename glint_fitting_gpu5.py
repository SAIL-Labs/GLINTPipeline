# -*- coding: utf-8 -*-
"""
Created on Thu Mar 14 15:44:07 2019

@author: mamartinod

Fit model of pdf over measured Na PDF

Requirements:
    - measured values of Na
    - measured values of photometries
    - measured values of dark noise
    - gaussian distribution of phase
    
To do:
    - generate random values from arbitrary PDF for photometries
    - generate random values from arbitrary PDF for dark current
    - compute a sample of Null values from the previous rv
    - create the histogram to fit to the measured Null PDF
"""

import numpy as np
import cupy as cp
import matplotlib.pyplot as plt
from scipy.optimize import curve_fit, least_squares
from timeit import default_timer as time
import h5py
import os
import sys
import glint_fitting_functions5 as gff
from scipy.special import erf
from scipy.io import loadmat
from scipy.stats import norm, skewnorm
import pickle
from datetime import datetime


def MCfunction(bins0, na, mu_opd, sig_opd):
    '''
    For now, this function deals with polychromatic for one baseline
    '''
    # Imported in the funcion
    global data_IA_axis, cdf_data_IA, data_IB_axis, cdf_data_IB, spectra # On GPU
    global zeta_minus_A, zeta_minus_B, zeta_plus_A, zeta_plus_B, data_IA, data_IB
    global dark_Iminus_cdf, dark_Iminus_axis, dark_Iplus_cdf, dark_Iplus_axis # On GPU
    global spec_chan_width, activate_use_photometry
    
    # generated by the function
    global rv_IA, rv_IB, rv_opd, rv_dark_Iminus, rv_dark_Iplus, rv_null, rv_interfminus, rv_interfplus, interfminus, interfplus # On GPU
    global phase_bias, bins
    global interfminus_binned, interfplus_binned

    # Parameters of the MC process
    global n_samp_per_loop, count, wl_scale0, nloop 
    global oversampling_switch, switch_invert_null
    global fichier
    global rv_IA_list

    # Test
    global Iplus, rv_interfplus, liste_rv_interfminus, liste_rv_interfplus, liste_rv_dark_Iminus, liste_rv_dark_Iplus
    
    count += 1
    print(int(count), na, mu_opd, sig_opd)
    try:
        fichier.write('%s\t%s\t%s\t%s\n'%(int(count), na, mu_opd, sig_opd))
    except:
        pass
    
    accum = cp.zeros((bins0.shape[0], bins0.shape[1]-1), dtype=cp.float32) # Axes: spectral channel, number of bins

    
    if wl_scale0.size > 1:
        spec_chan_width = abs(np.mean(np.diff(wl_scale0)))
    else:
        spec_chan_width = 5
    
        
    ''' Number of samples to simulate is high and the memory is low so we iterate to create an average histogram '''
    for _ in range(nloop):
        liste_rv_interfminus = cp.zeros((wl_scale0.size, n_samp_per_loop), dtype=cp.float32)
        liste_rv_interfplus = cp.zeros((wl_scale0.size, n_samp_per_loop), dtype=cp.float32)
        liste_rv_dark_Iminus = cp.zeros((wl_scale0.size, n_samp_per_loop), dtype=cp.float32)
        liste_rv_dark_Iplus = cp.zeros((wl_scale0.size, n_samp_per_loop), dtype=cp.float32)

        rv_opd = cp.random.normal(mu_opd, sig_opd, n_samp_per_loop)
        rv_opd = rv_opd.astype(cp.float32)
#        rv_opd = skewnorm.rvs(skew, loc=mu_opd, scale=sig_opd, size=n_samp_per_loop) # Skewd Gaussian distribution
#        rv_opd = cp.asarray(rv_opd, dtype=cp.float32) # Load the random values into the graphic card
        
        if activate_spectral_binning:
            interfminus_binned, interfplus_binned = cp.zeros(n_samp_per_loop, dtype=cp.float32), cp.zeros(n_samp_per_loop, dtype=cp.float32)
        
        # Generate random values of injection
        if not activate_use_photometry:
            rv_injectionA = gff.rv_generator(data_IA_axis, cdf_data_IA, n_samp_per_loop)   # random values for photometry A         
            rv_injectionB = gff.rv_generator(data_IB_axis, cdf_data_IB, n_samp_per_loop) # random values for photometry B
        
        for k in range(wl_scale0.size): # Iterate over the wavelength axis
#            bin_width = cp.mean(cp.diff(bins), dtype=cp.float32)
            # random values for dark noise
            rv_dark_Iminus = gff.rv_generator(dark_Iminus_axis[k], dark_Iminus_cdf[k], n_samp_per_loop)
            rv_dark_Iminus = rv_dark_Iminus.astype(cp.float32)
            rv_dark_Iplus = gff.rv_generator(dark_Iplus_axis[k], dark_Iplus_cdf[k], n_samp_per_loop)
            rv_dark_Iplus = rv_dark_Iplus.astype(cp.float32)
            # rv_dark_Iminus -= rv_dark_Iminus.mean()
            # rv_dark_Iplus -= rv_dark_Iplus.mean()            
            
            if not activate_use_photometry:
                rv_IA = rv_injectionA * spectra[0][k]
                rv_IB = rv_injectionB * spectra[1][k]
            else:
                rv_IA = cp.asarray(data_IA, dtype=cp.float32)
                rv_IB = cp.asarray(data_IB, dtype=cp.float32)
            
            # Random values for synthetic null depths
#            rv_null, rv_interfminus, rv_interfplus = gff.computeNullDepthLinear(na, rv_IA, rv_IB, wl_scale0[k], rv_opd, phase_bias, dphase_bias, rv_dark_Iminus, rv_dark_Iplus, 
#                     zeta_minus_A[k], zeta_minus_B[k], zeta_plus_A[k], zeta_plus_B[k],
#                     spec_chan_width, oversampling_switch, switch_invert_null)
            rv_null, rv_interfminus, rv_interfplus = gff.computeNullDepth(na, rv_IA, rv_IB, wl_scale0[k], rv_opd, phase_bias, dphase_bias, rv_dark_Iminus, rv_dark_Iplus, 
                     zeta_minus_A[k], zeta_minus_B[k], zeta_plus_A[k], zeta_plus_B[k],
                     spec_chan_width, oversampling_switch, switch_invert_null)
            
            if activate_spectral_binning:
                interfminus_binned += rv_interfminus
                interfplus_binned += rv_interfplus
            else:
                rv_null = rv_null[~np.isnan(rv_null)] # Remove NaNs
                rv_null = cp.sort(rv_null)
                
                ''' Compute the average histogram over the nloops iterations '''
                bins = cp.asarray(bins0[k], dtype=cp.float32)
                pdf_null = cp.histogram(rv_null, bins)[0]
                accum[k] += pdf_null / cp.sum(pdf_null)#*bin_width)
                
            ''' Save some debug stuff '''
            liste_rv_interfminus[k] = rv_interfminus
            liste_rv_interfplus[k] = rv_interfplus
            liste_rv_dark_Iminus[k] = rv_dark_Iminus
            liste_rv_dark_Iplus[k] = rv_dark_Iplus
                
        if activate_spectral_binning: 
#            interfminus_binned = interfminus_binned / wl_scale0.size
#            interfplus_binned = interfplus_binned / wl_scale0.size
            if switch_invert_null:
                rv_null = interfplus_binned / interfminus_binned
            else:
                rv_null = interfminus_binned / interfplus_binned

            rv_null = rv_null[~np.isnan(rv_null)] # Remove NaNs
            rv_null = cp.sort(rv_null)                
            pdf_null = cp.histogram(rv_null, cp.asarray(bins0[0], dtype=cp.float32))[0]
            accum[0] += pdf_null / cp.sum(pdf_null)#*bin_width)
        
    accum = accum / nloop
    if cp.all(cp.isnan(accum)):
        accum[:] = 0
    accum = cp.asnumpy(accum)

    return accum.ravel()

class Logger(object):
    ''' Class allowing to save the content of the console inside a txt file '''
    def __init__(self, log_path):
        self.orig_stdout = sys.stdout
        self.terminal = sys.stdout
        self.log = open(log_path, "a")

    def write(self, message):
        self.terminal.write(message)
        self.log.write(message)  

    def flush(self):
        #this flush method is needed for python 3 compatibility.
        #this handles the flush command by doing nothing.
        #you might want to specify some extra behavior here.
        pass
    
    def close(self):
        sys.stdout = self.orig_stdout
        self.log.close()
        print('Stdout closed')

def basin_hoppin_values(mu_opd0, sig_opd0, na0, bounds_mu, bounds_sig, bounds_na):
    ''' Create as many as initial guess as there are basin hopping iterations to do'''

#    orig_seed = np.random.get_state()
#    np.random.seed(1)
    print('Random withdrawing of init guesses')
    
    for _ in range(1000):
        mu_opd = np.random.normal(mu_opd0, 10)
        if mu_opd > bounds_mu[0] and mu_opd < bounds_mu[1]:
            break
        if _ == 1000-1:
            print('mu_opd: no new guess, take initial one')
            mu_opd = mu_opd0
    
    for _ in range(1000):
        sig_opd = abs(np.random.normal(sig_opd0, 10))
        if sig_opd > bounds_sig[0] and sig_opd < bounds_sig[1]:
            break
        if _ == 1000-1:
            print('sig opd: no new guess, take initial one')
            sig_opd = sig_opd0
        
    for _ in range(1000):
        na = np.random.normal(na0, 0.005)
        if na > bounds_na[0] and na < bounds_na[1]:
            break
        if _ == 1000-1:
            print('na: no new guess, take initial one')
            na = na0
            
    print('Random drawing done')
#    np.random.set_state(orig_seed)
    return mu_opd, sig_opd, na
        

''' Settings '''  
wl_min = 1525 # lower bound of the bandwidth to process
wl_max = 1575
wl_mid = (wl_max + wl_min)/2 # Centre wavelength of the bandwidth
n_samp_total = int(1e+8)
n_samp_per_loop = int(1e+7) # number of samples per loop
nloop = n_samp_total // n_samp_per_loop
mode = 'cuda' # Mode for using the MC method, let as 'cuda'
phase_bias_switch = True # Implement a non-null achromatic phase in the null model
opd_bias_switch = True # Implement an offset OPD in the null model
zeta_switch = True # Use the measured zeta coeff. If False, value are set to 0.5
oversampling_switch = True # Include the loss of coherence when OPD is too far from 0, according to a sinc envelop
skip_fit = True # Do not fit, plot the histograms of data and model given the initial guesses
chi2_map_switch = False # Map the parameters space over astronull, DeltaPhi mu and sigma
nb_files_data = (0, 1000) #(20782, None) # Which data files to load
nb_files_dark = (0, 1000) # Which dark files to load
basin_hopping_nloop = (0, 1) # lower and upper bound of the iteration loop for basin hopping method
activate_random_init_guesses = True
activate_spectral_binning = False
activate_use_photometry = False
activate_dark_correction = False
activate_save_basic_esti = False
which_nulls = ['null1', 'null4', 'null5', 'null6'][1:2]
map_na_sz = 10
map_mu_sz = 200
map_sig_sz = 10
nb_rows_plot = 3

# =============================================================================
# 20191212/lab_static
# =============================================================================
''' Set the bounds of the parameters to fit '''
nulls_to_invert = [''] # If one null and antinull outputs are swapped in the data processing
nulls_to_invert_model = [''] # If one null and antinull outputs are swapped in the data processing
bounds_mu0  = [(0, 600),      (2200, 2500), (2200, 2500), (100, 600), (4500, 5000), (0, 700)] # bounds for DeltaPhi mu, one tuple per null
bounds_sig0 = [(0, 500),    (200, 300),   (200, 300),   (100,400),      (100, 500),  (100, 400)] # bounds for DeltaPhi sig
#    bounds_sig0 = [(100, 400),    (200, 300),   (200, 300),   (50,300),      (100, 500),  (100, 400)] # ron 1271
bounds_na0  = [(-1, 0.01),   (-0.2, 0.2),  (-0.2, 0.2),  (-0.01, 0.01), (-0.05, 0.05), (-0.01, 0.01)] # bounds for astronull
#    bounds_mu0[4] = (-6600, -5900)
#    bounds_sig0[4] = (200, 500)
diffstep = [0.001, 10, 10] # differential step to apply to the TRF fitting algorithm, used for computing the finite difference
xscale = np.ones(len(diffstep)) # scale factor of the parameters to fit, see least_squares doc for more details
bin_bounds0 = [(-0.02, 0.2), (-0.1, 0.4), (-0.1, 0.4), (-0.02, 0.3), (-0.02, 0.1), (-0.02, 0.2)] # Boundaries of the histogram, to be set manually after checking the histogram sphape with "skip_fit = True"
#bin_bounds0 = [(-0.25, 1), (-0.1, 0.4), (-0.1, 0.4), (-0.25, 1), (-0.02, 0.1), (-0.02, 0.2)] # 459
#    bin_bounds0 = [(-0.6, 1.2), (-0.1, 0.4), (-0.1, 0.4), (-0.6, 1.2), (-0.02, 0.1), (-0.02, 0.2)] # 1271

''' Set the initial conditions '''
mu_opd0 = np.array([200, 2400, 2400, 300, 4700, 400]) # initial guess of DeltaPhi mu
sig_opd0 = np.array([200, 260, 260, 160, 300, 250]) # initial guess of DeltaPhi sig
na0 = np.array([0., 0, 0, 0, 0, 0]) # initial guess of astro null

''' Import real data '''
datafolder = '20191212_raw_sum/'
darkfolder = '20191212_raw_sum/'
# root = "//silo.physics.usyd.edu.au/silo4/snert/"
# root = "C:/Users/marc-antoine/glint/"
root = "/mnt/96980F95980F72D3/glint/"
file_path = root+'GLINTprocessed/'+datafolder
save_path = file_path+'output_test/'
data_list = [file_path+f for f in os.listdir(file_path) if '.hdf5' in f and 'lab_turb_01' in f][nb_files_data[0]:nb_files_data[1]]
#data_list = [file_path+f for f in os.listdir(file_path) if '.hdf5' in f and 'n5n6' in f][nb_files_data[0]:nb_files_data[1]]
dark_list = [root+'GLINTprocessed/'+darkfolder+f for f in os.listdir(root+'GLINTprocessed/'+darkfolder) if '.hdf5' in f and 'dark_01' in f][nb_files_dark[0]:nb_files_dark[1]]


# =============================================================================
# Rock 'n roll
# =============================================================================
if len(data_list) == 0 or len(dark_list) == 0:
    raise UserWarning('data list or dark list is empty')
    
calib_params_path = file_path#root+'GLINTprocessed/calibration_params/'
zeta_coeff_path = calib_params_path + '20200106_zeta_coeff_raw.hdf5'

if not os.path.exists(save_path):
    os.makedirs(save_path)

''' Some constants/useless variables kept for retrocompatibility '''
dphase_bias = 0. # constant value for corrective phase term
phase_bias = 0. # Phase of the fringes
    
# List in dictionary: indexes of null, beam A and beam B, zeta label for antinull, segment id
null_table = {'null1':[0,[0,1], 'null7'], 'null2':[1,[1,2], 'null8'], 'null3':[2,[0,3], 'null9'], \
              'null4':[3,[2,3], 'null10'], 'null5':[4,[2,0], 'null11'], 'null6':[5,[3,1], 'null12']}

''' Specific settings for some configurations '''
if chi2_map_switch:
    n_samp_per_loop = int(1e+5) # number of samples per loop
    nloop = 1
    basin_hopping_nloop = (basin_hopping_nloop[0], basin_hopping_nloop[0]+1)
    
if skip_fit:
    basin_hopping_nloop = (basin_hopping_nloop[0], basin_hopping_nloop[0]+1)
 
    
''' Fool-proof '''
if not chi2_map_switch and not skip_fit:
    check_mu =  np.any(mu_opd0 <= np.array(bounds_mu0)[:,0]) or np.any(mu_opd0 >= np.array(bounds_mu0)[:,1])
    check_sig =  np.any(sig_opd0 <= np.array(bounds_sig0)[:,0]) or np.any(sig_opd0 >= np.array(bounds_sig0)[:,1])
    check_null = np.any(na0 <= np.array(bounds_na0)[:,0]) or np.any(na0 >= np.array(bounds_na0)[:,1])
    
    if check_mu or check_sig or check_null:
        raise Exception('Check boundaries: the initial guesses (marked as True) are not between the boundaries (null:%s, mu:%s, sig:%s).'%(check_null, check_mu, check_sig))
    
total_time_start = time()
for key in which_nulls: # Iterate over the null to fit
    # =============================================================================
    # Import data
    # =============================================================================
    print('****************')
    print('Processing %s \n'%key)
    
    plt.ioff()
    
    if key in nulls_to_invert_model:
        switch_invert_null = True
    else:
        switch_invert_null = False    
   
    ''' Load data about the null to fit '''
    dark = gff.load_data(dark_list, (wl_min, wl_max), key, nulls_to_invert)
    data = gff.load_data(data_list, (wl_min, wl_max), key, nulls_to_invert, dark)
        
    wl_scale0 = data['wl_scale'] # Wavelength axis. One histogrm per value in this array will be created. The set of histogram will be fitted at once.
            
    ''' Load the zeta coeff we need. if "wl_bounds" kew is sent, the return zeta coeff are the average over the bandwidth set by the tuple of this key'''
    zeta_coeff = gff.get_zeta_coeff(zeta_coeff_path, wl_scale0, False)
    if not zeta_switch:
        for zkey in zeta_coeff.keys():
            if zkey != 'wl_scale':
                zeta_coeff[zkey][:] = 1.
    
    ''' Get histograms of intensities and dark current in the pair of photomoetry outputs '''
    idx_null = null_table[key][0] # Select the index of the null output to process
    idx_photo = null_table[key][1] # Select the indexes of the concerned photometries
    key_antinull = null_table[key][2] # Select the index of the antinull output to process
    data_IA, data_IB = data['photo'][0], data['photo'][1] # Set photometries in dedicated variable into specific variables for clarity. A and B are the generic id of the beams for the processed baseiune
    dark_IA, dark_IB = dark['photo'][0], dark['photo'][1]

    zeta_minus_A, zeta_minus_B = zeta_coeff['b%s%s'%(idx_photo[0]+1, key)], zeta_coeff['b%s%s'%(idx_photo[1]+1, key)] # Set zeta coeff linking null and photometric outputs into dedicated variables for clarity
    zeta_plus_A, zeta_plus_B = zeta_coeff['b%s%s'%(idx_photo[0]+1, key_antinull)], zeta_coeff['b%s%s'%(idx_photo[1]+1, key_antinull)] # Set zeta coeff linking antinull and photometric outputs into dedicated variables for clarity

       
    # =============================================================================
    # Get CDF of dark and photometries
    # =============================================================================
    ''' Get CDF of dark currents in interferometric outputs for generating random values in the MC function '''
    dark_size = [len(np.linspace(dark['Iminus'][i].min(), dark['Iminus'][i].max(), \
                             np.size(np.unique(dark['Iminus'][i])), endpoint=False)) for i in range(len(wl_scale0))]
    dark_Iminus_axis = cp.array([np.linspace(dark['Iminus'][i].min(), dark['Iminus'][i].max(), \
                                             min(dark_size), endpoint=False) for i in range(len(wl_scale0))], \
                                             dtype=cp.float32)

    dark_Iminus_cdf = cp.array([cp.asnumpy(gff.computeCdf(dark_Iminus_axis[i], dark['Iminus'][i], 'cdf', True)) \
                                for i in range(len(wl_scale0))], dtype=cp.float32)

    dark_size = [len(np.linspace(dark['Iplus'][i].min(), dark['Iplus'][i].max(), \
                             np.size(np.unique(dark['Iminus'][i])), endpoint=False)) for i in range(len(wl_scale0))]

    dark_Iplus_axis = cp.array([np.linspace(dark['Iplus'][i].min(), dark['Iplus'][i].max(), \
                                            min(dark_size), endpoint=False) for i in range(len(wl_scale0))], \
                                            dtype=cp.float32)

    dark_Iplus_cdf = cp.array([cp.asnumpy(gff.computeCdf(dark_Iplus_axis[i], dark['Iplus'][i], 'cdf', True)) \
                               for i in range(len(wl_scale0))], dtype=cp.float32)


    ''' Handling photometry: either get the CDF for rv generation or just keep the temporal sequence '''
    if not activate_use_photometry:
        injection, spectra = gff.getInjectionAndSpectrum(data_IA, data_IB, wl_scale0)
        injection = np.array(injection)
        
        if activate_dark_correction:
            injection_dark, spectra_dark = gff.getInjectionAndSpectrum(dark_IA, dark_IB, wl_scale0)
            injection_dark  = np.array(injection_dark)
            injection_saved = injection.copy()
            mean_data, var_data = np.mean(injection, axis=-1), np.var(injection, axis=-1)
            mean_dark, var_dark = np.mean(injection_dark, axis=-1), np.var(injection_dark, axis=-1)
            
            injection = (injection - mean_data[:,None]) * \
                ((var_data[:,None]-var_dark[:,None])/var_data[:,None])**0.5 + mean_data[:,None] - mean_dark[:,None]
                
            if np.any(np.isnan(injection)) or np.any(np.isinf(injection)):
                print('Restore injection')
                injection = injection_saved.copy()
        
        data_IA_axis = cp.linspace(injection[0].min(), injection[0].max(), np.size(np.unique(injection[0])), dtype=cp.float32)
        cdf_data_IA = gff.computeCdf(data_IA_axis, injection[0], 'cdf', True)
        cdf_data_IA = cp.array(cdf_data_IA, dtype=cp.float32)
    
        data_IB_axis = cp.linspace(injection[1].min(), injection[1].max(), np.size(np.unique(injection[1])), dtype=cp.float32)
        cdf_data_IB = gff.computeCdf(data_IB_axis, injection[1], 'cdf', True)
        cdf_data_IB = cp.array(cdf_data_IB, dtype=cp.float32)
    else:
        n_samp_per_loop = data_IA.shape[1]
        nloop = max((n_samp_total // n_samp_per_loop, 1))
        
    # =============================================================================
    # Compute the null
    # =============================================================================
    if activate_spectral_binning:
        Iminus = gff.binning(data['Iminus'], data['Iminus'].shape[0], 0)
        Iplus = gff.binning(data['Iplus'], data['Iplus'].shape[0], 0)
        wl_scale = gff.binning(wl_scale0, wl_scale0.size, 0, True)
    else:
        Iminus = data['Iminus']
        Iplus = data['Iplus']
        wl_scale = wl_scale0

        
    if key in nulls_to_invert:
        data_null = Iplus / Iminus
    else:
        data_null = Iminus / Iplus
    
    # =============================================================================
    # Compute the histogram        
    # =============================================================================
    print('Compute survival function and error bars')
    bin_bounds = bin_bounds0[idx_null]

    sz = np.array([np.size(d[(d>=bin_bounds[0])&(d<=bin_bounds[1])]) for d in data_null]) # size of the sample of measured null depth.
    sz = np.max(sz) # size of the sample of measured null depth.
#    sz = 1000**2
    ''' Creation of the x-axis of the histogram (one per wavelength)'''
    null_axis = np.array([np.linspace(bin_bounds[0], bin_bounds[1], int(sz**0.5+1), retstep=False, dtype=np.float32) for i in range(data_null.shape[0])])
    
    null_pdf = []
    null_pdf_err = []
    for wl in range(len(wl_scale)): # Compute the histogram per spectral channel
        ''' Create the histogram (one per wavelegnth) '''
        pdf = np.histogram(data_null[wl], null_axis[wl], density=False)[0]
        pdf_size = np.sum(pdf)
        bin_width = null_axis[wl][1]-null_axis[wl][0]
        pdf = pdf / np.sum(pdf)
        null_pdf.append(pdf)
        
        # pdf_err = gff.getErrorPDF(data_null[wl], data_null_err[wl], null_axis[wl]) # Barnaby's method, tend to underestimate the error
        pdf_err = gff.getErrorBinomNorm(pdf, pdf_size) # Classic method
        null_pdf_err.append(pdf_err)
                                    
    null_pdf = np.array(null_pdf)
    null_pdf_err = np.array(null_pdf_err)
    
    # =============================================================================
    # Basic estimation for classic calibration    
    # =============================================================================
    if activate_save_basic_esti:
        data_null2 = [d[d>=0] for d in data_null]
        rms = [d.std() for d in data_null2]
        data_null3 = [data_null2[k][(data_null2[k]>=data_null2[k].min())&(data_null2[k]<=data_null2[k].min()+rms[k])] for k in range(len(data_null2))]
        avg = np.array([np.mean(elt) for elt in data_null3])
        std = np.array([np.std(elt) for elt in data_null3])
        
        basic_esti = np.array([avg, std, wl_scale])
        basic_save = save_path+'basic_esti_'+key+'_'+str(wl_min)+'-'+str(wl_max)+'_'+os.path.basename(datafolder[:-1])
        if activate_spectral_binning:
            basic_save = basic_save+'_sp'
        np.save(basic_save, basic_esti)
        continue
    
    # =============================================================================
    # Section where the fit is done.        
    # =============================================================================
    ''' Select the bounds of the baseline (null) to process '''
    bounds_mu = bounds_mu0[idx_null]
    bounds_sig = bounds_sig0[idx_null]
    bounds_na = bounds_na0[idx_null]
    # Compile them into a readable tuple called by the TRF algorithm
    bounds_fit = ([bounds_na[0], bounds_mu[0], bounds_sig[0]], 
                  [bounds_na[1], bounds_mu[1], bounds_sig[1]])
    
    wl_scale_saved = wl_scale.copy()
    for idx_basin, basin_hopping_count in enumerate(range(basin_hopping_nloop[0], basin_hopping_nloop[1])):
        if not skip_fit and not chi2_map_switch:
            plt.close('all')
            
        if not chi2_map_switch:
            sys.stdout = Logger(save_path+'basin_hop_%s_%02d'%(key, basin_hopping_count)+'.log') # Save the content written in the console into a txt file

        chi2_liste = [] # Save the reduced Chi2 of the different basin hop
        popt_liste = [] # Save the optimal parameters of the different basin hop
        uncertainties_liste = [] # Save the errors on fitted parameters of the different basin hop
        init_liste = [] # Save the initial guesses of the different basin hop
        pcov_liste = [] # Save the covariance matrix given by the fitting algorithm of the different basin hop
        termination_liste = [] # Save the termination condition of the different basin hop
            
        print('-------------')
        print(basin_hopping_count)
        print('-------------')
        print('Fitting '+key)  
        # model fitting initial guess

        ''' Create the set of initial guess for each hop '''
        if idx_basin > 0 and activate_random_init_guesses:
            mu_opd, sig_opd, na = basin_hoppin_values(mu_opd0[idx_null], sig_opd0[idx_null], na0[idx_null], bounds_mu, bounds_sig, bounds_na)
        else:
            mu_opd = mu_opd0[idx_null]
            sig_opd = sig_opd0[idx_null]
            na = na0[idx_null]
            start_basin_hoppin = True

                
        ''' Model fitting '''
        if not chi2_map_switch:
            guess_na = na
            initial_guess = [guess_na, mu_opd, sig_opd]
            initial_guess = np.array(initial_guess, dtype=np.float64)
            
            if skip_fit:
                ''' No fit is perforend here, just load the values in the initial guess and compute the histogram'''
                print('Direct display')
                count = 0.
                start = time()
                out = MCfunction(null_axis, *initial_guess)
                stop = time()
                print('Duration:', stop-start)
#                out = z.reshape(null_cdf.shape)
                na_opt = na
                uncertainties = np.zeros(3)
                popt = (np.array([na, mu_opd, sig_opd]), np.ones((3,3)))
                chi2 = 1/(null_pdf.size-popt[0].size) * np.sum((null_pdf.ravel() - out)**2/null_pdf_err.ravel()**2)
                term_status = None
                print('chi2', chi2)
            
            else:            
                ''' Fit is done here '''
                print('Model fitting')    
                count = 0.
                init_liste.append(initial_guess)
                
                start = time()
                ''' Save the content of the console generated by this function into a txt file'''
                with open(save_path+'callfunc_%02d.txt'%(basin_hopping_count), 'a') as fichier:
                    popt = gff.curvefit(MCfunction, null_axis, null_pdf.ravel(), p0=initial_guess, sigma=null_pdf_err.ravel(), 
                                        bounds=bounds_fit, diff_step = diffstep, x_scale=xscale)
                    
                res = popt[2] # all outputs of the fitting function but optimal parameters and covariance matrix (see scipy.optimize.minimize doc)
                popt = popt[:2] # Optimal parameters found
                print('Termination:', res.message) # Termination condition
                term_status = res.status # Value associated by the termination condition
                
                stop = time()
                print('Termination', term_status)
                print('Duration:', stop - start)

                out = MCfunction(null_axis, *popt[0]) # Hsigogram computed according to the optimal parameters
                uncertainties = np.diag(popt[1])**0.5 # Errors on the optimal parameters
                chi2 = 1/(null_pdf.size-popt[0].size) * np.sum((null_pdf.ravel() - out)**2/null_pdf_err.ravel()**2) # Reduced Chi2
                print('chi2', chi2)
                
                ''' Display in an easy-to-read way this key information (optimal parameters, error and reduced Chi2) '''
                na_opt = popt[0][0]
                print('******')
                print(popt[0])
                print(uncertainties*chi2**0.5)
                print(chi2)
                print('******')

                ''' Save input and the outputs of the fit into a npz file. One per basin hop '''
                np.savez(save_path+os.path.basename(file_path[:-1])+'_%s_%03d'%(key, basin_hopping_count),
                         chi2=chi2, popt=[na_opt]+[elt for elt in popt[0][1:]], uncertainties=uncertainties, init=[guess_na]+list(initial_guess[1:]),
                                         termination=np.array([term_status]), nsamp=np.array([n_samp_per_loop]), wl=wl_scale)
            
            chi2_liste.append(chi2)
            popt_liste.append([na_opt]+[elt for elt in popt[0][1:]])
            uncertainties_liste.append(uncertainties)
            termination_liste.append(term_status)
            pcov_liste.append(popt[1])
            
            ''' Display the results of the fit'''
            # Each figure display the histogram of 10 wavelength, if more than 10 are fitted, extra figures are created                    
            wl_idx0 = np.arange(wl_scale.size)
            wl_idx0 = list(gff.divide_chunks(wl_idx0, nb_rows_plot*2)) # Subset of wavelength displayed in one figure
            
            for wl_idx in wl_idx0:
                f = plt.figure(figsize=(19.20,10.80))
                txt3 = '%s '%key+'Fitted values: ' + 'Na$ = %.2E \pm %.2E$, '%(na_opt, uncertainties[0]) + \
                r'$\mu_{OPD} = %.2E \pm %.2E$ nm, '%(popt[0][1], uncertainties[1]) + \
                r'$\sigma_{OPD} = %.2E \pm %.2E$ nm,'%(popt[0][2], uncertainties[2])+' Chi2 = %.2E '%(chi2)+'(Last = %.3f s)'%(stop-start)
#                txt3 = '%s '%key+'Fitted values: ' + 'Na$ = %.2E \pm %.2E$, '%(na_opt, uncertainties[0]) +' Chi2 = %.2E '%(chi2)+'(Last = %.3f s)'%(stop-start)
                count = 0
                axs = []
                for wl in wl_idx[::-1]:
                    if len(wl_idx) > 1:
                        ax = f.add_subplot(nb_rows_plot,2,count+1)
                    else:
                        ax = f.add_subplot(1,1,count+1)
                    axs.append(ax)
                    plt.title('%s nm'%wl_scale[wl], size=20)
                    plt.errorbar(null_axis[wl][:-1], null_pdf[wl], yerr=null_pdf_err[wl], fmt='.', markersize=5, label='Data')
                    plt.errorbar(null_axis[wl][:-1], out.reshape((wl_scale.size,-1))[wl], markersize=5, lw=3, alpha=0.8, label='Fit')                    
                    plt.grid()
#                    plt.legend(loc='best', fontsize=25)
                    if list(wl_idx).index(wl) <= 1 or len(wl_idx) == 1: plt.xlabel('Null depth', size=20)
                    if count %2 == 0: plt.ylabel('Frequency', size=20)
                    plt.xticks(size=15);plt.yticks(size=15)
#                    plt.ylim(0, 0.003)
                    # plt.xlim(-0.01, 0.5)
                    count += 1
                plt.tight_layout(rect=[0., 0.05, 1, 1])
                if len(wl_idx) > 1:
                    axs[0].text(0.5, -6.9, txt3, va='center', transform = axs[0].transAxes, bbox=dict(boxstyle="square", facecolor='white'))
                else:
                    axs[0].text(0.3, -0.1, txt3, va='center', transform = axs[0].transAxes, bbox=dict(boxstyle="square", facecolor='white'))
                string = key+'_'+'%03d'%(basin_hopping_count)+'_'+str(wl_min)+'-'+str(wl_max)+'_'+os.path.basename(datafolder[:-1])+'_%s'%int(wl_scale[wl_idx[-1]])
                if not oversampling_switch: string = string + '_nooversamplinginmodel'
                if not zeta_switch: string = string + '_nozetainmodel'
                if not skip_fit: 
                    string = string + '_fit_pdf'
                if activate_spectral_binning: string = string + '_sb'
                plt.savefig(save_path+string+'.png')

            ''' Display the details of the fit: make sure the reconstructed null and antinull match with the real ones '''
            histom = [np.histogram(Iminus[k], bins=int(Iminus[k].size**0.5), density=True) for k in range(wl_scale.size)]
            histom2 = [np.histogram(cp.asnumpy(liste_rv_interfminus[k]), bins=int(liste_rv_interfminus[k].size**0.5), density=True) for k in range(wl_scale.size)]
            histop = [np.histogram(Iplus[k], bins=int(Iplus[k].size**0.5), density=True) for k in range(wl_scale.size)]
            histop2 = [np.histogram(cp.asnumpy(liste_rv_interfplus[k]), bins=int(liste_rv_interfplus[k].size**0.5), density=True) for k in range(wl_scale.size)]
            histodkm = [np.histogram(dark['Iminus'][k], bins=int(dark['Iminus'][k].size**0.5), density=True) for k in range(wl_scale.size)]
            histodkp = [np.histogram(dark['Iplus'][k], bins=int(dark['Iplus'][k].size**0.5), density=True) for k in range(wl_scale.size)]
            
            for wl_idx in wl_idx0:
                f = plt.figure(figsize=(19.20,10.80))
                count = 0
                axs = []
                for wl in wl_idx[::-1]:
                    if len(wl_idx) > 1:
                        ax = f.add_subplot(nb_rows_plot,2,count+1)
                    else:
                        ax = f.add_subplot(1,1,count+1)
                    axs.append(ax)
                    plt.title('%s nm'%wl_scale[wl], size=20)
                    plt.plot(histom[wl][1][:-1], histom[wl][0], '.', label='Iminus')
                    plt.plot(histom2[wl][1][:-1], histom2[wl][0], label='rv minus')
                    plt.plot(histop[wl][1][:-1], histop[wl][0], '.', label='Iplus')
                    plt.plot(histop2[wl][1][:-1], histop2[wl][0], label='rv plus')
                    plt.plot(histodkm[wl][1][:-1], histodkm[wl][0], label='Dark minus')
                    plt.plot(histodkp[wl][1][:-1], histodkp[wl][0], label='Dark plus')
                    plt.grid()
                    plt.xticks(size=15);plt.yticks(size=15)
                    if count %2 == 0: plt.ylabel('Frequency', size=20)
                    if count == 0: plt.legend(loc='best', ncol=3, fontsize=15)
                    if list(wl_idx).index(wl) <= 1 or len(wl_idx) == 1: plt.xlabel('Flux (AU)', size=20)
                    count += 1
                plt.tight_layout()
                string = key+'_details_'+'%03d'%(basin_hopping_count)+'_'+str(wl_min)+'-'+str(wl_max)+'_'+os.path.basename(datafolder[:-1])+'_%s'%int(wl_scale[wl_idx[-1]])
                if not oversampling_switch: string = string + '_nooversamplinginmodel'
                if not zeta_switch: string = string + '_nozetainmodel'
                if not skip_fit: 
                    string = string + '_fit_pdf'
                if activate_spectral_binning: string = string + '_sb'
                plt.savefig(save_path+string+'.png')
                
            ''' Plot the histogram of the photometries '''
            ''' Photo A '''
            for wl_idx in wl_idx0:
                f = plt.figure(figsize=(19.20,10.80))
                axs = []
                count = 0
                for wl in wl_idx[::-1]:
                    histo_IA = np.histogram(data_IA[wl], int(data_IA[wl].size**0.5), density=True)
                    histo_dIA = np.histogram(dark['photo'][0][wl], int(np.size(dark['photo'][0][wl])**0.5), density=True)
                    if len(wl_idx) > 1:
                        ax = f.add_subplot(nb_rows_plot,2,count+1)
                    else:
                        ax = f.add_subplot(1,1,count+1)
                    axs.append(ax)
                    plt.title('%s nm'%wl_scale[wl], size=20)
                    plt.plot(histo_IA[1][:-1], histo_IA[0], '.', markersize=5, label='P%s'%(null_table[key][1][0]+1))
                    plt.plot(histo_dIA[1][:-1], histo_dIA[0], '.', markersize=5, label='Dark')
                    plt.grid()
                    plt.legend(loc='best', fontsize=15)
                    plt.xlabel('Flux (AU)', size=20)
                    plt.ylabel('Frequency', size=20)
                    plt.xticks(size=15);plt.yticks(size=20)
                    count += 1
                plt.tight_layout()
                string = 'P%s'%(null_table[key][1][0]+1)+'_'+key+'_'+'%03d'%(basin_hopping_count)+'_'+str(wl_min)+'-'+str(wl_max)+'_'+os.path.basename(datafolder[:-1])+'_%s'%int(wl_scale[wl_idx[-1]])
                if activate_spectral_binning: string = string + '_sb'
                plt.savefig(save_path+string+'.png')

            ''' Photo B '''
            for wl_idx in wl_idx0:
                f = plt.figure(figsize=(19.20,10.80))
                axs = []
                count = 0
                for wl in wl_idx[::-1]:
                    histo_IB = np.histogram(data_IB[wl], int(data_IB[wl].size**0.5), density=True)
                    histo_dIB = np.histogram(dark['photo'][1][wl], int(np.size(dark['photo'][1][wl])**0.5), density=True)
                    if len(wl_idx) > 1:
                        ax = f.add_subplot(nb_rows_plot,2,count+1)
                    else:
                        ax = f.add_subplot(1,1,count+1)
                    axs.append(ax)
                    plt.title('%s nm'%wl_scale[wl], size=20)
                    plt.plot(histo_IB[1][:-1], histo_IB[0], '.', markersize=5, label='P%s'%(null_table[key][1][1]+1))
                    plt.plot(histo_dIB[1][:-1], histo_dIB[0], '.', markersize=5, label='Dark')
                    plt.grid()
                    plt.legend(loc='best', fontsize=15)
                    plt.xlabel('Flux (AU)', size=20)
                    plt.ylabel('Frequency', size=20)
                    plt.xticks(size=15);plt.yticks(size=15)
                    count += 1
                plt.tight_layout()
                string = 'P%s'%(null_table[key][1][1]+1)+'_'+key+'_'+'%03d'%(basin_hopping_count)+'_'+str(wl_min)+'-'+str(wl_max)+'_'+os.path.basename(datafolder[:-1])+'_%s'%int(wl_scale[wl_idx[-1]])
                if activate_spectral_binning: string = string + '_sb'
                plt.savefig(save_path+string+'.png')
        else:
            ''' Map the parameters space '''
            print('Mapping parameters space')
            count = 0
            map_na, step_na = np.linspace(bounds_na[0], bounds_na[1], map_na_sz, endpoint=False, retstep=True)
            map_mu_opd, step_mu = np.linspace(bounds_mu[0], bounds_mu[1], map_mu_sz, endpoint=False, retstep=True)
            map_sig_opd, step_sig = np.linspace(bounds_sig[0], bounds_sig[1], map_sig_sz, endpoint=False, retstep=True)
            chi2map = []
            start = time()
            for visi in map_na:
                temp1 = []
                for o in map_mu_opd:
                    temp2 = []
                    for s in map_sig_opd:
                        parameters = np.array([visi, o, s])
                        out = MCfunction(null_axis, *parameters)
                        value = 1/(null_pdf.size-parameters.size) * np.sum((null_pdf.ravel() - out)**2/null_pdf_err.ravel()**2)                        
                        temp2.append([value, visi, o, s])
                    temp1.append(temp2)
                chi2map.append(temp1)
            stop = time()
            chi2map = np.array(chi2map)
            print('Duration: %.3f s'%(stop-start))
            
            if activate_spectral_binning:
                chi2_savepath = save_path+'chi2map_%s_%03d_%s-%s_sp'%(key, basin_hopping_count, int(wl_min), int(wl_max))
            else:
                chi2_savepath = save_path+'chi2map_%s_%03d_%s-%s'%(key, basin_hopping_count, int(wl_min), int(wl_max))
            np.savez(chi2_savepath, value=chi2map, na=map_na, mu=map_mu_opd, sig=map_sig_opd, wl=wl_scale)
            
            chi2map2 = chi2map[:,:,:,0]
            chi2map2[np.isnan(chi2map2)] = np.nanmax(chi2map[:,:,:,0])
            argmin = np.unravel_index(np.argmin(chi2map2), chi2map2.shape)
            print('Min in param space', chi2map2.min(), map_na[argmin[0]], map_mu_opd[argmin[1]], map_sig_opd[argmin[2]])
            print('Indexes are:', argmin)
            fake = chi2map2.copy()
            fake[argmin] = chi2map2.max()
            argmin2 = np.unravel_index(np.argmin(fake), chi2map2.shape)
            print('2nd min in param space', chi2map2[argmin2], map_na[argmin2[0]], map_mu_opd[argmin2[1]], map_sig_opd[argmin2[2]])
            print('Indexes are:', argmin2)
                
            valmin = np.nanmin(chi2map[:,:,:,0])
            valmax = np.nanmax(chi2map[:,:,:,0])

            ''' plot the 3D parameters space '''
            # WARNING: they are in log scale
            plt.figure(figsize=(19.20,10.80))
            if map_na.size > 10:
                iteration = np.arange(argmin[0]-5,argmin[0]+5)
                if np.min(iteration) < 0:
                    iteration -= iteration.min()
            else:
                iteration = np.arange(map_na.size)
            for i, it in zip(iteration, range(10)):
                plt.subplot(5,2,it+1)
                plt.imshow(np.log10(chi2map[i,:,:,0].T), interpolation='none', origin='lower', aspect='auto', 
                           extent=[map_mu_opd[0]-step_mu/2, map_mu_opd[-1]+step_mu/2, map_sig_opd[0]-step_sig/2, map_sig_opd[-1]+step_sig/2],
                           vmin=np.log10(valmin), vmax=np.log10(valmax))
                plt.colorbar()
                plt.ylabel('sig opd');plt.xlabel('mu opd')
                plt.title('Na %s'%map_na[i])
            plt.tight_layout(rect=[0,0,1,0.95])
            plt.suptitle(key)
            if activate_spectral_binning:
                plt.savefig(save_path+'chi2map_%s_%03d_mu_vs_sig_%s-%s_sp.png'%(key, basin_hopping_count, int(wl_min), int(wl_max)))
            else:
                plt.savefig(save_path+'chi2map_%s_%03d_mu_vs_sig_%s-%s.png'%(key, basin_hopping_count, int(wl_min), int(wl_max)))
            
            plt.figure(figsize=(19.20,10.80))
            if map_sig_opd.size >10:
                iteration = np.arange(argmin[2]-5,argmin[2]+5)
                if np.min(iteration) < 0:
                    iteration -= iteration.min()
            else:
                iteration = np.arange(map_sig_opd.size)
            for i, it in zip(iteration, range(10)):
                plt.subplot(5,2,it+1)
                plt.imshow(np.log10(chi2map[:,:,i,0]), interpolation='none', origin='lower', aspect='auto', 
                           extent=[map_mu_opd[0]-step_mu/2, map_mu_opd[-1]+step_mu/2, map_na[0]-step_na/2, map_na[-1]+step_na/2],
                           vmin=np.log10(valmin), vmax=np.log10(valmax))
                plt.colorbar()
                plt.title('sig %s'%map_sig_opd[i])
                plt.xlabel('mu opd');plt.ylabel('null depth')
            plt.tight_layout(rect=[0,0,1,0.95])
            plt.suptitle(key)
            if activate_spectral_binning:
                plt.savefig(save_path+'chi2map_%s_%03d_null_vs_mu_%s-%s_sp.png'%(key, basin_hopping_count, int(wl_min), int(wl_max)))
            else:
                plt.savefig(save_path+'chi2map_%s_%03d_null_vs_mu_%s-%s.png'%(key, basin_hopping_count, int(wl_min), int(wl_max)))
                
            plt.figure(figsize=(19.20,10.80))
            if map_mu_opd.size > 10:
                iteration = np.arange(argmin[1]-5,argmin[1]+5)
                if np.min(iteration) < 0:
                    iteration -= iteration.min()
            else:
                iteration = np.arange(map_mu_opd.size)
            for i, it in zip(iteration, range(10)):
                plt.subplot(5,2,it+1)
                plt.imshow(np.log10(chi2map[:,i,:,0]), interpolation='none', origin='lower', aspect='auto', 
                           extent=[map_sig_opd[0]-step_sig/2, map_sig_opd[-1]+step_sig/2, map_na[0]-step_na/2, map_na[-1]]+step_na/2,
                           vmin=np.log10(valmin), vmax=np.log10(valmax))
                plt.colorbar()
                plt.xlabel('sig opd');plt.ylabel('null depth')    
                plt.title('mu %s'%map_mu_opd[i])
            plt.tight_layout(rect=[0,0,1,0.95])
            plt.suptitle(key)
            if activate_spectral_binning:
                plt.savefig(save_path+'chi2map_%s_%03d_null_vs_sig_%s-%s_sp.png'%(key, basin_hopping_count, int(wl_min), int(wl_max)))
            else:
                plt.savefig(save_path+'chi2map_%s_%03d_null_vs_sig_%s-%s.png'%(key, basin_hopping_count, int(wl_min), int(wl_max)))

            
            with open(save_path+'mapping_%s_%03d'%(key, basin_hopping_count)+'.log', 'w') as map_log:
                map_log.write('******* Mapping %s *******\n'%key)
                map_log.write('Initial conditions (na, mu, sig):\n')
                map_log.write(str(bounds_na)+str(bounds_mu)+str(bounds_sig)+'\n')
                map_log.write('Sizes:\n')
                map_log.write('%s\t%s\t%s\n'%(map_na_sz, map_mu_sz, map_sig_sz)+'\n\n')
                map_log.write('Results\n')
                map_log.write('Min in param space\n%s\t%s\t%s\t%s\n'%(chi2map2.min(), map_na[argmin[0]], map_mu_opd[argmin[1]], map_sig_opd[argmin[2]]))
                map_log.write('Indexes are\n'+str(argmin)+'\n')                
                map_log.write('2nd min in param space\n%s\t%s\t%s\t%s\n'%(chi2map2[argmin2], map_na[argmin2[0]], map_mu_opd[argmin2[1]], map_sig_opd[argmin2[2]]))
                map_log.write('Indexes are\n'+str(argmin2)+'\n')
                map_log.write('Duration: %.3f s\n'%(stop-start))
                map_log.write('----- End -----\n\n')
                print('Text saved')
                
        if not chi2_map_switch:
            sys.stdout.close()
        
        results = {key:[popt_liste, uncertainties_liste, chi2_liste, init_liste, termination_liste, pcov_liste, wl_scale, n_samp_per_loop]}
        wl_scale = wl_scale_saved
    
        if not skip_fit and not chi2_map_switch:
            ''' Save the optimal parameters, inputs, fit information of all basin hop in one run '''
            pickle_name = key+'_'+'%03d'%(basin_hopping_count)+'_'+str(wl_min)+'-'+str(wl_max)+'_'+os.path.basename(datafolder[:-1])
            if activate_spectral_binning:
                pickle_name = pickle_name + '_sb'
            pickle_name = pickle_name+'_pdf'
            pickle_name = pickle_name+'.pkl'
                                               
            with open(save_path+pickle_name, 'wb') as f:
                pickle.dump(results, f)
                print('--- Pickle saved ---')
            
total_time_stop = time()            
print('Total time', total_time_stop-total_time_start)
plt.ion()
plt.show()



print('-- End --')

# interfminus_binned = cp.asnumpy(interfminus_binned)
# interfminus_binned = interfminus_binned[~np.isnan(interfminus_binned)]
# interfplus_binned = cp.asnumpy(interfplus_binned)
# interfplus_binned = interfplus_binned[~np.isnan(interfplus_binned)]

# histom = np.histogram(Iminus[0], bins=int(Iminus.size**0.5), density=True)
# histom2 = np.histogram(interfminus_binned, bins=int(interfminus_binned.size**0.5), density=True)
# histop = np.histogram(Iplus[0], bins=int(Iplus.size**0.5), density=True)
# histop2 = np.histogram(interfplus_binned, bins=int(interfplus_binned.size**0.5), density=True)

# histodkA = np.histogram(dark_IA.sum(axis=0), bins=int(np.size(dark_IA.sum(axis=0))**0.5), density=True)
# histodkB = np.histogram(dark_IB.sum(axis=0), bins=int(np.size(dark_IB.sum(axis=0))**0.5), density=True)
# histoA = np.histogram(data_IA.sum(axis=0), bins=int(np.size(data_IA.sum(axis=0))**0.5), density=True)
# histoB = np.histogram(data_IB.sum(axis=0), bins=int(np.size(data_IB.sum(axis=0))**0.5), density=True)

# opd_rv = np.random.normal(387, 20, data_IA.shape[1])
# # darkm_rv = np.random.normal(dark['Iminus'].mean(), dark['Iminus'].std(), data_IA.shape)
# # darkp_rv = np.random.normal(dark['Iplus'].mean(), dark['Iplus'].std(), data_IA.shape)
# darkm_rv = np.array([np.random.normal(dark['Iminus'][k].mean(), dark['Iminus'][k].std(), data_IA[k].size) for k in range(wl_scale.size)])
# darkp_rv = np.array([np.random.normal(dark['Iplus'][k].mean(), dark['Iplus'][k].std(), data_IA[k].size) for k in range(wl_scale.size)])

# data_IA0, data_IB0 = data_IA.copy(), data_IB.copy()
# mean_dataA, var_dataA = np.mean(data_IA, axis=-1), np.var(data_IA, axis=-1)
# mean_darkA, var_darkA = np.mean(dark['photo'][0], axis=-1), np.var(dark['photo'][0], axis=-1)

# data_IA = (data_IA - mean_dataA[:,None]) * \
#     ((var_dataA[:,None]-var_darkA[:,None])/var_darkA[:,None])**0.5 + mean_dataA[:,None] - mean_darkA[:,None]

# mean_dataB, var_dataB = np.mean(data_IB, axis=-1), np.var(data_IB, axis=-1)
# mean_darkB, var_darkB = np.mean(dark['photo'][1], axis=-1), np.var(dark['photo'][1], axis=-1)

# data_IB = (data_IB - mean_dataB[:,None]) * \
#     ((var_dataB[:,None]-var_darkB[:,None])/var_darkB[:,None])**0.5 + mean_dataB[:,None] - mean_darkB[:,None]



# reconstruct_minus = (data_IA) * zeta_minus_A[:,None] + (data_IB) * zeta_minus_B[:,None] - \
#                 2 * ((data_IA) * zeta_minus_A[:,None] * (data_IB) * zeta_minus_B[:,None])**0.5 * np.sin(2*np.pi/(wl_scale0[:,None])*opd_rv) + darkm_rv

# reconstruct_plus = (data_IA) * zeta_plus_A[:,None] + (data_IB) * zeta_plus_B[:,None] +\
#                 2 * ((data_IA) * zeta_plus_A[:,None] * (data_IB) * zeta_plus_B[:,None])**0.5 * np.sin(2*np.pi/(wl_scale0[:,None])*opd_rv) + darkp_rv

# reconstruct_minus = reconstruct_minus.sum(axis=0)             
# reconstruct_minus = reconstruct_minus[~np.isnan(reconstruct_minus)]

# reconstruct_plus = reconstruct_plus.sum(axis=0)
# reconstruct_plus = reconstruct_plus[~np.isnan(reconstruct_plus)]

# histo_reconm = np.histogram(reconstruct_minus, histom[1], density=True)
# histo_reconp = np.histogram(reconstruct_plus, histop[1], density=True)

# plt.figure(figsize=(19.20,10.80))
# plt.plot(histom[1][:-1], histom[0], '.', label='Iminus')
# plt.plot(histom2[1][:-1], histom2[0], label='rv minus')
# plt.plot(histop[1][:-1], histop[0], '.', label='Iplus')
# plt.plot(histop2[1][:-1], histop2[0], label='rv plus')
# plt.plot(histo_reconm[1][:-1], histo_reconm[0], '--', label='recons -')
# plt.plot(histo_reconp[1][:-1], histo_reconp[0], '--', label='recons +')
# plt.plot(histodkA[1][:-1], histodkA[0], label='Dark A')
# plt.plot(histodkB[1][:-1], histodkB[0], label='Dark B')
# plt.plot(histoA[1][:-1], histoA[0], label='P A')
# plt.plot(histoB[1][:-1], histoB[0], label='P B')
# plt.legend(loc='best')
# plt.grid()
# plt.xlim(-300, 4000)
# plt.ylim(-1e-3, 0.033)
# #plt.ylim(5e-5, 1.4e-3)
# plt.tight_layout()

# plt.figure()
# plt.plot(histodkA[1][:-1]-np.mean(dark_IA.sum(axis=0)), histodkA[0], label='Dark A')
# plt.plot(histodkB[1][:-1]-np.mean(dark_IB.sum(axis=0)), histodkB[0], label='Dark B')
# plt.plot(histoA[1][:-1]-np.mean(data_IA.sum(axis=0)), histoA[0], label='P A')
# plt.plot(histoB[1][:-1]-np.mean(data_IB.sum(axis=0)), histoB[0], label='P B')
# plt.legend(loc='best')





liste_rv_interfminus = cp.asnumpy(liste_rv_interfminus)
liste_rv_interfplus = cp.asnumpy(liste_rv_interfplus)

if activate_spectral_binning:
    liste_rv_interfminus = np.sum(liste_rv_interfminus, axis=0).reshape(1,-1)
    liste_rv_interfplus = np.sum(liste_rv_interfplus, axis=0).reshape(1,-1)

liste_rv_interfminus = [liste_rv_interfminus[k][~np.isnan(liste_rv_interfminus[k])] for k in range(wl_scale.size)]
liste_rv_interfplus = [liste_rv_interfplus[k][~np.isnan(liste_rv_interfplus[k])] for k in range(wl_scale.size)]

histom = [np.histogram(Iminus[k], bins=int(Iminus[k].size**0.5), density=True) for k in range(wl_scale.size)]
histom2 = [np.histogram(liste_rv_interfminus[k], bins=int(liste_rv_interfminus[k].size**0.5), density=True) for k in range(wl_scale.size)]
histop = [np.histogram(Iplus[k], bins=int(Iplus[k].size**0.5), density=True) for k in range(wl_scale.size)]
histop2 = [np.histogram(liste_rv_interfplus[k], bins=int(liste_rv_interfplus[k].size**0.5), density=True) for k in range(wl_scale.size)]

histodkA = [np.histogram(dark_IA[k], bins=int(np.size(dark_IA[k])**0.5), density=True) for k in range(wl_scale.size)]
histodkB = [np.histogram(dark_IB[k], bins=int(np.size(dark_IB[k])**0.5), density=True) for k in range(wl_scale.size)]
histoA = [np.histogram(data_IA[k], bins=int(np.size(data_IA[k])**0.5), density=True) for k in range(wl_scale.size)]
histoB = [np.histogram(data_IB[k], bins=int(np.size(data_IB[k])**0.5), density=True) for k in range(wl_scale.size)]

darkm_rv = np.array([np.random.normal(dark['Iminus'][k].mean(), dark['Iminus'][k].std(), data_IA[k].size) for k in range(wl_scale.size)])
darkp_rv = np.array([np.random.normal(dark['Iplus'][k].mean(), dark['Iplus'][k].std(), data_IA[k].size) for k in range(wl_scale.size)])
darkm_rv = np.random.normal(dark['Iminus'].mean(), dark['Iminus'].std(), data_IA.shape)
darkp_rv = np.random.normal(dark['Iplus'].mean(), dark['Iplus'].std(), data_IA.shape)
darkm_rv = cp.asnumpy(liste_rv_dark_Iminus[:,:data_IA.shape[1]])
darkp_rv = cp.asnumpy(liste_rv_dark_Iplus[:,:data_IA.shape[1]])

if activate_dark_correction:
    data_IA0, data_IB0 = data_IA.copy(), data_IB.copy()
    mean_dataA, var_dataA = np.mean(data_IA, axis=-1), np.var(data_IA, axis=-1)
    mean_darkA, var_darkA = np.mean(dark['photo'][0], axis=-1), np.var(dark['photo'][0], axis=-1)
    
    data_IA = (data_IA - mean_dataA[:,None]) * \
        ((var_dataA[:,None]-var_darkA[:,None])/var_darkA[:,None])**0.5 + mean_dataA[:,None] - mean_darkA[:,None]
    
    mean_dataB, var_dataB = np.mean(data_IB, axis=-1), np.var(data_IB, axis=-1)
    mean_darkB, var_darkB = np.mean(dark['photo'][1], axis=-1), np.var(dark['photo'][1], axis=-1)
    
    data_IB = (data_IB - mean_dataB[:,None]) * \
        ((var_dataB[:,None]-var_darkB[:,None])/var_darkB[:,None])**0.5 + mean_dataB[:,None] - mean_darkB[:,None]

opd_rv = np.random.normal(popt[0][1], popt[0][2], data_IA.shape[1])
reconstruct_minus = (data_IA) * zeta_minus_A[:,None] + (data_IB) * zeta_minus_B[:,None] - \
                2 * ((data_IA) * zeta_minus_A[:,None] * (data_IB) * zeta_minus_B[:,None])**0.5 * np.sin(2*np.pi/(wl_scale0[:,None])*opd_rv) + darkm_rv

reconstruct_plus = (data_IA) * zeta_plus_A[:,None] + (data_IB) * zeta_plus_B[:,None] +\
                2 * ((data_IA) * zeta_plus_A[:,None] * (data_IB) * zeta_plus_B[:,None])**0.5 * np.sin(2*np.pi/(wl_scale0[:,None])*opd_rv) + darkp_rv

if activate_spectral_binning:
    reconstruct_minus = reconstruct_minus.sum(axis=0).reshape(1,-1)           
    reconstruct_plus = reconstruct_plus.sum(axis=0).reshape(1,-1)

reconstruct_minus = [reconstruct_minus[k][~np.isnan(reconstruct_minus[k])] for k in range(wl_scale.size)]
reconstruct_plus = [reconstruct_plus[k][~np.isnan(reconstruct_plus[k])] for k in range(wl_scale.size)]

histo_reconm = [np.histogram(reconstruct_minus[k], histom[k][1], density=True) for k in range(wl_scale.size)]
histo_reconp = [np.histogram(reconstruct_plus[k], histop[k][1], density=True) for k in range(wl_scale.size)]


for wl_idx in wl_idx0:
    f = plt.figure(figsize=(19.20,10.80))
    count = 0
    axs = []
    for wl in wl_idx[::-1]:
        if len(wl_idx) > 1:
            ax = f.add_subplot(nb_rows_plot,2,count+1)
        else:
            ax = f.add_subplot(1,1,count+1)
        axs.append(ax)
        plt.title('%s nm'%wl_scale[wl], size=15)
        plt.plot(histom[wl][1][:-1], histom[wl][0], '.', label='Iminus')
        plt.plot(histom2[wl][1][:-1], histom2[wl][0], label='rv minus')
        plt.plot(histop[wl][1][:-1], histop[wl][0], '.', label='Iplus')
        plt.plot(histop2[wl][1][:-1], histop2[wl][0], label='rv plus')
        plt.plot(histo_reconm[wl][1][:-1], histo_reconm[wl][0], '--', label='recons -')
        plt.plot(histo_reconp[wl][1][:-1], histo_reconp[wl][0], '--', label='recons +')
        plt.plot(histodkA[wl][1][:-1], histodkA[wl][0], label='Dark A')
        plt.plot(histodkB[wl][1][:-1], histodkB[wl][0], label='Dark B')
        plt.plot(histoA[wl][1][:-1], histoA[wl][0], label='P A')
        plt.plot(histoB[wl][1][:-1], histoB[wl][0], label='P B')
        plt.grid()
        if count == 0: plt.legend(loc='best', ncol=3)
        count += 1
    plt.tight_layout()
